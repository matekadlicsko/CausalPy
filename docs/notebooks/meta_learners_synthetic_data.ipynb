{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fafe7c80-fc2d-47d4-b260-c5a1082d6fbb",
   "metadata": {},
   "source": [
    "# Comparing meta-learners with different ensembles of trees as base learners"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ccc6766-7606-4b6b-8c76-b4b5b732c988",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Meta-learners are a class of models used for estimating *heterogeneous causal effect* of a certain treatement. Here by heterogeneity, we mean that the effect of the treatement may vary across individuals. They approach this problem by decomposing it to several subregressions. The models used for these subregressions are called *base learners*. Some of the most popular base learners are tree based ensembles, in particular Random Forest and Bayesian Additive Regression Trees (BART) due to their flexibility, however any machine learning algorithm suitable for regression problems (like generalized linear regressions, gaussian processes, etc.) can be used.\n",
    "\n",
    "We will denote by $Y$ a $d$-dimensional outcome vector, by $X \\in \\mathbb{R}^{d \\times n}$ a feature matrix containing potential confounders, by $W \\in \\{0, 1\\}^d$ the treatment assignment indicator and by  $Y(0) \\in \\mathbb{R}^d$ and $Y(1) \\in \\mathbb{R}^d$ the potential outcome of corresponding units when assigned to the control and the treatement group respectively. Note that in general, we only observe one of the potential outcomes, furthermore\n",
    "$$ Y = W Y(1) + (1 - W) Y(0).$$\n",
    "\n",
    "Our task is to estimate the *conditional average treatement effect (CATE)*, defined as \n",
    "$$\\tau(x) = \\mathbb{E}\\left[Y(1) - Y(0) \\mid X=x\\right].$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed73c7c9-7df5-4912-80f4-c5e69c916c22",
   "metadata": {},
   "source": [
    "## Data generation\n",
    "\n",
    "In this notebook we will deal with synthetic data, so that we can quantify exactly the mean squared error of our CATE estimations. The data generating is of the form\n",
    "$$y = \\operatorname{sinc}(\\alpha X) + W \\sigma(\\beta X) + \\varepsilon,$$\n",
    "where $\\varepsilon_i \\sim N(0, 1)$ and $W_i \\sim \\operatorname{Bernoulli}\\left(\\frac14\\right)$ are i.i.d., $$\\sigma(x) = \\frac{e^x}{1 + e^x}, \\quad \\text{and} \\quad \\operatorname{sinc}(x) = \\frac{\\sin(x)}{x}$$ are applied coordinate-wise.\n",
    "\n",
    "Here, CATE can be expressed as $$\\tau(X) = \\sigma(\\beta X).$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c543b717-acb7-4c7f-aa77-975833dbf699",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from causalpy.skl_meta_learners import SLearner, TLearner, XLearner, DRLearner\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from xgboost import XGBClassifier\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return np.exp(x) / (1 + np.exp(x))\n",
    "\n",
    "def sinc(x):\n",
    "    return np.where(x==0, 1, np.sin(x) / x)\n",
    "\n",
    "def summarize_learner(learner):\n",
    "    learner.summary(n_iter=100)\n",
    "    print(f\"Actual average treatement effect: {treatment_effect.mean()}\")\n",
    "    print(f\"MSE: {mean_squared_error(treatment_effect, learner.cate)}\")\n",
    "\n",
    "\n",
    "def create_synthetic_data(*shape):\n",
    "    X = np.random.rand(*shape)\n",
    "\n",
    "    α = np.random.rand(shape[1])\n",
    "    β = np.random.rand(shape[1])\n",
    "\n",
    "    treatment = np.random.binomial(n=1, p=.25, size=shape[0])\n",
    "    treatment_effect = sigmoid(X @ β)\n",
    "\n",
    "    noise = np.random.rand(shape[0])\n",
    "\n",
    "    y = sinc(X @ α) + treatment_effect * treatment + noise\n",
    "\n",
    "    X = pd.DataFrame(X, columns=[f'feature_{i}' for i in range(shape[1])])\n",
    "    y = pd.Series(y, name='target')\n",
    "    treated = pd.Series(treatment, name='treatment')\n",
    "    return X, y, treated, treatment_effect\n",
    "\n",
    "X, y, treated, treatment_effect = create_synthetic_data(500, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf837e1-d93c-4fc5-bc52-96296db82e51",
   "metadata": {
    "tags": []
   },
   "source": [
    "## S-learner\n",
    "\n",
    "The simplest meta-learner is the *S-learner*, which predicts the potential outcome vectors by treating the treatement assignment indicator as a feature and estimates\n",
    "\n",
    "$$\\mu(x, w) := \\mathbb{E}[Y \\mid X=x, W=w]$$\n",
    "\n",
    "with an appropriate base learner. The \"S\" in S-learner stands for \"single\", as it uses a single model to estimate CATE. We will denote this estimation with $\\hat{\\mu}(x, w)$. Then the S-learner approximates CATE as  \n",
    "\n",
    "$$\\hat{\\tau_S}(x) = \\hat{\\mu}(x, 1) - \\hat{\\mu}(x, 0).$$\n",
    "\n",
    "In the following example we will choose `HistGradientBoostingRegressor` as base learner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454322d0-6dbc-4771-bb7a-344afa30e793",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Utility for printing some statistics.\n",
    "slearner = SLearner(\n",
    "    X=X,\n",
    "    y=y,\n",
    "    treated=treated,\n",
    "    model=HistGradientBoostingRegressor()\n",
    ")\n",
    "\n",
    "summarize_learner(slearner)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d35f7f-6526-4b7d-ac36-2365fc08cb5d",
   "metadata": {},
   "source": [
    "## T-learner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a8971c-5eef-4423-893e-9277e8a40ac1",
   "metadata": {},
   "source": [
    "The *T-learner* is a closely related meta-learner. Here, instead of including the treatment assignment indicator as a feature, we estimate the potential outcomes in two different regressions. Namely, we estimate\n",
    "$$\\mu_{\\operatorname{treated}}(x) := \\mathbb{E}[Y(1) \\mid X=x], \\quad \\text{and} \\quad \\mu_{\\operatorname{untreated}}(x) := \\mathbb{E}[Y(0) \\mid X=x]$$\n",
    "separately and define estimate the cate as\n",
    "$$\\hat{\\tau}_T(x) = \\hat{\\mu}_{\\operatorname{treated}}(x) - \\hat{\\mu}_{\\operatorname{untreated}}(x),$$\n",
    "where $\\hat{\\mu}_{\\operatorname{treated}}$ and $\\hat{\\mu}_{\\operatorname{untreated}}$ are the estimated functions. \n",
    "\n",
    "If we choose our base-learners to be a forest based ensemble, in the case of the S-learner, during the fitting process splits on the treatement assignment indicator can happen *anywhere* in the trees. In the fitting process of the T-learner however, we essentially force a split at the first level. This difference means that the S-learner is more flexible, but especially when using regularization, it is prone to disregard the effect of the treatment. On the other hand the T-learner uses two separate models that do not share information between each other, which may lead to overfitting if one of the treatement groups is small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a24a76e-398a-4629-9d49-7fadecb2a2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "tlearner = TLearner(\n",
    "    X=X,\n",
    "    y=y,\n",
    "    treated=treated,\n",
    "    model=HistGradientBoostingRegressor()\n",
    ")\n",
    "\n",
    "summarize_learner(tlearner)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bef7acd-3d29-4b2a-b364-a915a30f4f46",
   "metadata": {},
   "source": [
    "# X-learner\n",
    "Similarly, to the T-learner, the X-learner first estimates $\\mu_{\\operatorname{treated}}$ and $\\mu_{\\operatorname{untreated}}$. Next, it computes \n",
    "\n",
    "$$D(1) := Y(1) - \\hat{\\mu}_{\\operatorname{untreated}}(X(1)), \\quad \\text{and} \\quad D(0) = \\hat{\\mu}_{\\operatorname{treated}}(X(0)) - Y(0).$$\n",
    "\n",
    "Note that if $\\hat{\\mu}_{\\operatorname{untreated}} = \\mu_{\\operatorname{untreated}}$ and $\\hat{\\mu}_{\\operatorname{treated}} = \\mu_{\\operatorname{treated}}$, then $\\tau(x) = \\mathbb{E}[D(1) | X=x] = \\mathbb{E}[D(0) | X=x]$, so CATE may be estimated by regressing on $D(0)$ or $D(1)$. Let us denote the estimations obtained this way by $\\tau_0$ and $\\tau_1$, respectively. \n",
    "\n",
    "Finally, the X-learner computes a weighted average of the two CATE estimates, that is \n",
    "$$\\hat{\\tau}_X(x) = g(x)\\hat{\\tau}_0 + (1 - g(x))\\hat{\\tau_1}(x),$$\n",
    "for some weight function $g: \\mathbb{R}^d \\rightarrow [0, 1]$.\n",
    "\n",
    "In CausalPy, we obtain $g$ as an estimation of the so-called *propensity score*, that is the conditional probability of a unit receiving treatment given $X$. By default, we use logistic regression to obtain this estimation. However, in case one of the treatement groups is small, it makes sense to set $g \\equiv 0$ or $g \\equiv 1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250bd516-b1ad-452f-b26c-e0eafc3747f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Using logistic regression based propensity score\n",
    "xlearner = XLearner(\n",
    "    X=X,\n",
    "    y=y,\n",
    "    treated=treated,\n",
    "    model=HistGradientBoostingRegressor()\n",
    ")\n",
    "\n",
    "summarize_learner(xlearner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaab018b-c95e-4c29-9163-410e39cd34de",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Using constant g\n",
    "xlearner_mf = XLearner(\n",
    "    X=X,\n",
    "    y=y,\n",
    "    treated=treated,\n",
    "    model=HistGradientBoostingRegressor(),\n",
    "    propensity_score_model=DummyClassifier(strategy=\"most_frequent\")\n",
    ")\n",
    "\n",
    "summarize_learner(xlearner_mf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a499cc5-2fcf-4016-978b-c3c28c861d5a",
   "metadata": {},
   "source": [
    "# DR-learner\n",
    "The *doubly robust learner*, also known as the *DR-learner*, is the last CATE estimator that we are going to look at."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687aa584-b8af-4863-a7cf-570e86080057",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "drlearner = DRLearner(\n",
    "    X=X,\n",
    "    y=y,\n",
    "    treated=treated,\n",
    "    model=HistGradientBoostingRegressor(),\n",
    "    propensity_score_model=XGBClassifier()\n",
    ")\n",
    "\n",
    "summarize_learner(drlearner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3552a561-8051-4a51-832a-5e088a106689",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor, HistGradientBoostingRegressor, AdaBoostRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.svm import SVR\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "def compute_mse(learner, m):\n",
    "    l = learner(X, y, treated, m)\n",
    "    return mean_squared_error(treatment_effect, l.cate)\n",
    "\n",
    "models = [\n",
    "    (\"Random forest\", RandomForestRegressor()),\n",
    "    (\"Hist gradient boosting\", HistGradientBoostingRegressor()),\n",
    "    (\"AdaBoost\", AdaBoostRegressor()),\n",
    "    (\"XGBoost\", XGBRegressor()),\n",
    "    (\"LightGBM\", LGBMRegressor()),\n",
    "    (\"SVR\", SVR())\n",
    "]\n",
    "\n",
    "learners = [\n",
    "    (\"S-learner\", SLearner),\n",
    "    (\"T-learner\", TLearner),\n",
    "    (\"X-learner\", XLearner),\n",
    "    (\"DR-learner\", DRLearner)\n",
    "]\n",
    "bench = pd.DataFrame({\n",
    "    m_name: {\n",
    "        learner_name:  compute_emse(learner, m) for learner_name, learner in learners\n",
    "    } for m_name, m in models\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eed6094-c940-4b5b-8294-bd2ffeab2eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "sns.heatmap(bench, annot=True, ax=ax)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0df81690-aba8-4f1c-9e2a-feb016664fca",
   "metadata": {},
   "source": [
    "Bibliography:\n",
    "[1] Künzel, Sören R., Jasjeet S. Sekhon, Peter J. Bickel, and Bin Yu.\n",
    "    Metalearners for estimating heterogeneous treatment effects using machine learning.\n",
    "    Proceedings of the national academy of sciences 116, no. 10 (2019): 4156-4165."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
